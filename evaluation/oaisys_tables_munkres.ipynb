{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79663f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from deeplab.oaisys_utils import OAISYS_LABELS\n",
    "import semsegcluster.data.scannet\n",
    "from IPython.display import display\n",
    "\n",
    "from semsegcluster.oaisys_eval_munkres import get_measurements, get_ood\n",
    "from deeplab.oaisys_ood_detection import get_measurements as get_measurements_ood\n",
    "from semsegcluster.settings import EXP_OUT\n",
    "from semsegcluster.data.nyu_depth_v2 import TRAINING_LABEL_NAMES, NYU40_COLORS\n",
    "from semsegcluster.sacred_utils import get_incense_loader\n",
    "from semsegcluster.segmentation_metrics import SegmentationMetric\n",
    "from deeplab.oaisys_utils import OAISYS_LABELS\n",
    "import detectron2 as det2\n",
    "import detectron2.utils.visualizer\n",
    "# loader = get_incense_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='oaisys_trajectory_grass'\n",
    "methods=['label','pred395-merged-ensemble_pred-seg122','merged-ensemble_pred-seg122', 'ensemble_pred', 'seg122', 'pred400-merged-ensemble_pred-seg122', 'pred403-merged-ensemble_pred-seg122', 'dino365']\n",
    "methods=['label','merged-ensemble_pred-seg122', 'ensemble_pred', 'pred','seg122', 'pred400-merged-ensemble_pred-seg122', 'pred403-merged-ensemble_pred-seg122', 'dino365']\n",
    "\n",
    "OAISYS_LABELS[15] = 'Grass-Field'\n",
    "m = get_measurements(subset=subset, pretrained_id='deeplab', methods=methods)\n",
    "df = pd.DataFrame.from_dict({name: {\n",
    "    OAISYS_LABELS[l]: m[name]['assigned_iou'][l]\n",
    "    for l in range(37)\n",
    "    if not np.isnan(m['label']['assigned_iou'][l])\n",
    "} for name in m if not name == 'label'}).T\n",
    "df = df.fillna(0)\n",
    "df['mean'] = df.mean(axis=1)\n",
    "df['v_score'] = [m[name]['v_score'] for name in df.index]\n",
    "pd.set_option(\"display.float_format\", lambda f: f\"{f:0.2f}\")\n",
    "#print(df.to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n",
    "df.sort_values(by='mean', ascending=False)[:45]\n",
    "# print(df.sort_values(by='mean', ascending=False)[:45].to_latex(float_format=lambda x: f'{x * 100:.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='oaisys_trajectory3'\n",
    "methods=['label','pred396-merged-ensemble_pred-seg107','merged-ensemble_pred-seg107', 'ensemble_pred', 'seg107', 'pred401-merged-ensemble_pred-seg107', 'pred402-merged-ensemble_pred-seg107', 'dino368']\n",
    "methods=['label','merged-ensemble_pred-seg107', 'ensemble_pred', 'pred', 'seg107', 'pred401-merged-ensemble_pred-seg107', 'pred402-merged-ensemble_pred-seg107', 'dino368']\n",
    "OAISYS_LABELS[15] = 'Sand'\n",
    "m = get_measurements(subset=subset, pretrained_id='deeplab', methods=methods)\n",
    "df = pd.DataFrame.from_dict({name: {\n",
    "    OAISYS_LABELS[l]: m[name]['assigned_iou'][l]\n",
    "    for l in range(37)\n",
    "    if not np.isnan(m['label']['assigned_iou'][l])\n",
    "} for name in m if not name == 'label'}).T\n",
    "df = df.fillna(0)\n",
    "df['mean'] = df.mean(axis=1)\n",
    "df['v_score'] = [m[name]['v_score'] for name in df.index]\n",
    "pd.set_option(\"display.float_format\", lambda f: f\"{f:0.2f}\")\n",
    "#print(df.to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n",
    "df.sort_values(by='mean', ascending=False)[:45]\n",
    "# print(df.sort_values(by='mean', ascending=False)[:45].to_latex(float_format=lambda x: f'{x * 100:.0f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd00842",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='rugd'\n",
    "methods=['ensemble_label', 'ensemble_pred', 'pred402-rugd']\n",
    "OAISYS_LABELS[15] = 'Sand'\n",
    "m = get_measurements(subset=subset, pretrained_id='deeplab', methods=methods, only_newclass=True)\n",
    "df = pd.DataFrame.from_dict({name: {\n",
    "    OAISYS_LABELS[l]: m[name]['assigned_iou'][l]\n",
    "    for l in range(37)\n",
    "    if not np.isnan(m['ensemble_label']['assigned_iou'][l])\n",
    "} for name in m if not name == 'ensemble_label'}).T\n",
    "df = df.fillna(0)\n",
    "df['mean'] = df.mean(axis=1)\n",
    "df['v_score'] = [m[name]['v_score'] for name in df.index]\n",
    "pd.set_option(\"display.float_format\", lambda f: f\"{f:0.2f}\")\n",
    "#print(df.to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n",
    "df.sort_values(by='mean', ascending=False)[:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='rugd_sand'\n",
    "methods=['label', 'ensemble_pred', 'seg449', 'merged-ensemble_pred-seg449_-4.39', 'dino454', 'merged-ensemble_pred-dino454_-4.39', 'pred456-merged-ensemble_pred-dino454_-4.39']\n",
    "OAISYS_LABELS[15] = 'Sand'\n",
    "m = get_measurements(subset=subset, pretrained_id='deeplab', methods=methods, only_newclass=False)\n",
    "df = pd.DataFrame.from_dict({name: {\n",
    "    OAISYS_LABELS[l]: m[name]['assigned_iou'][l]\n",
    "    for l in range(37)\n",
    "    if not np.isnan(m['label']['assigned_iou'][l])\n",
    "} for name in m if not name == 'label'}).T\n",
    "df = df.fillna(0)\n",
    "df['mean'] = df.mean(axis=1)\n",
    "df['v_score'] = [m[name]['v_score'] for name in df.index]\n",
    "pd.set_option(\"display.float_format\", lambda f: f\"{f:0.2f}\")\n",
    "#print(df.to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n",
    "df.sort_values(by='mean', ascending=False)[:45]\n",
    "# print(df.sort_values(by='mean', ascending=False)[:45].to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e34e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset='oaisys_trajectory3'\n",
    "methods=['ensemble_maxlogit-pp']\n",
    "uncert_treshold = -5.5\n",
    "get_ood(subset=subset, pretrained_id='deeplab', methods=methods, uncert_treshold=uncert_treshold)\n",
    "# print('done')\n",
    "# print(measurements)\n",
    "# with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "#     print(pd.DataFrame.from_dict(measurements).T, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['rugd_sand', 'oaisys_trajectory_grass', 'oaisys_trajectory3']\n",
    "methods=['ensemble_maxlogit-pp', 'maxlogit-pp', 'norm_maxlogit-pp', 'gmm_max_log_prob_100_11']\n",
    "\n",
    "eval_values = ['AP', 'AUROC', 'ideal_threshold', 'Precision', 'Recall', 'Accuracy', 'F1']\n",
    "\n",
    "subsets = ['rugd_sand']\n",
    "eval_values = ['AP', 'AUROC', 'F1', 'ideal_threshold']\n",
    "\n",
    "\n",
    "\n",
    "# cen_input_measurements = {\n",
    "#     'oaisys_trajectory_grass': {\n",
    "#         'AUROC': 0.7814402430642234,\n",
    "#         'AP': 0.31355944696811,\n",
    "#     },\n",
    "#     'oaisys_trajectory3': {\n",
    "#         'AUROC': 0.4996484766932421,\n",
    "#         'AP': 0.38695004408845196,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# plot the results\n",
    "n_sub = 10000\n",
    "all_measurements = {}\n",
    "measurements = get_measurements_ood(subsets=subsets, pretrained_id='deeplab', methods=methods, only_newclass=True)\n",
    "for subset in subsets:\n",
    "    for method in methods:\n",
    "        try:\n",
    "            threshold = measurements[subset][method]['Threshold']\n",
    "        except:\n",
    "            continue\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        sens = measurements[subset][method]['TPR']\n",
    "        spec = 1 - measurements[subset][method]['FPR']\n",
    "        g_mean = np.sqrt(sens * spec)\n",
    "\n",
    "        ideal_index = np.argmax(g_mean)\n",
    "        ideal_threshold = threshold[ideal_index]\n",
    "        measurements[subset][method]['ideal_threshold'] = ideal_threshold\n",
    "        fp = measurements[subset][method]['FPR'][ideal_index]\n",
    "        measurements[subset][method]['FP'] = fp\n",
    "        tp = measurements[subset][method]['TPR'][ideal_index]\n",
    "        measurements[subset][method]['TP'] = tp\n",
    "        measurements[subset][method]['prescision'] = tp / (tp + fp)\n",
    "\n",
    "        fpr_sub = measurements[subset][method]['FPR'][0::n_sub]\n",
    "        tpr_sub = measurements[subset][method]['TPR'][0::n_sub]\n",
    "        sens_sub = sens[0::n_sub]\n",
    "        spec_sub = spec[0::n_sub]\n",
    "        g_mean_sub = g_mean[0::n_sub]\n",
    "        threshold_sub = threshold[0::n_sub]\n",
    "\n",
    "        ax[0].set_title(f'ROC Curve')\n",
    "        ax[0].set_xlabel('FPR')\n",
    "        ax[0].set_ylabel('TPR')\n",
    "        ax[0].plot(fpr_sub, tpr_sub, label='ROC')\n",
    "        ax[0].legend()\n",
    "        ax[1].set_title(f'Threshold evaluation')\n",
    "        ax[1].set_xlabel('Treshold')\n",
    "        ax[1].plot(threshold_sub, sens_sub, label='Sensitivity')\n",
    "        ax[1].plot(threshold_sub, spec_sub, label='Specificity')\n",
    "        ax[1].plot(threshold_sub, g_mean_sub, label='G-Mean')\n",
    "        ax[1].legend()\n",
    "        # add tilte to figure\n",
    "        fig.suptitle(f'{method}, dataset: {subset}')\n",
    "# cen_measurement = {\n",
    "#     'AUROC': cen_input_measurements[subset]['AUROC'],\n",
    "#     'AP': cen_input_measurements[subset]['AP'],\n",
    "# }\n",
    "# cen_measurements = {\n",
    "#     'CEN_paper': cen_measurement,\n",
    "# }\n",
    "# measurements.update(cen_measurements)\n",
    "# measurements['CEN_paper']['AUROC'] = 0.7\n",
    "# measurements['CEN_paper']['AUPR'] = 0.2\n",
    "# with pd.option_context('display.float_format', '{:0.4f}'.format):\n",
    "#     print(pd.DataFrame.from_dict(measurements).T, flush=True)\n",
    "\n",
    "for subset in subsets:\n",
    "    m = measurements[subset]\n",
    "    df = pd.DataFrame.from_dict({method: {\n",
    "        eval_value: m[method][eval_value]\n",
    "        for eval_value in eval_values\n",
    "    } for method in methods\n",
    "      if m[method]}).T\n",
    "    # df = df.fillna(0)\n",
    "    # df['mean'] = df.mean(axis=1)\n",
    "    # df['v_score'] = [m[name]['v_score'] for name in df.index]\n",
    "    pd.set_option(\"display.float_format\", lambda f: f\"{f:0.2f}\")\n",
    "    #print(df.to_latex(float_format=lambda x: f'{x * 100:.0f}'))\n",
    "    print(df.sort_values(by='AUROC',ascending=False).style.set_caption(subset).to_latex())\n",
    "\n",
    "df.sort_values(by='AUROC',ascending=False)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
